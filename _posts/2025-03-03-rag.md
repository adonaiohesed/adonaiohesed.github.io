---
title: Retrieval-Augmented Generation (RAG)
tags: RAG
key: page-rag
categories: [AI, GenAI]
author: hyoeun
math: true
mathjax_autoNumber: true
---

## Retrieval-Augmented Generation (RAG)

Large Language Models (LLMs) have demonstrated incredible capabilities, becoming a core technology for many services. However, LLMs have fundamental limitations, namely the **"Knowledge Cut-off"** and **"Hallucination"** phenomena. Models are unaware of information beyond their training date and can confidently generate plausible-sounding but false information.

From a software engineer's perspective, how can we solve this problem? The answer lies in **Retrieval-Augmented Generation (RAG)**, a powerful and efficient architecture that overcomes the limitations of LLMs by dynamically leveraging external knowledge sources.

### What is RAG? Understanding it with an "Open-Book Exam" Analogy 📝

The easiest way to understand RAG is to compare it to an open-book exam.

* **A standard LLM** is like a student taking a **"closed-book exam."** The student can answer questions they know flawlessly but has no choice but to guess or imagine answers for things they don't know (leading to hallucination).
* **A RAG-based LLM** is like a student taking an **"open-book exam."** The student can reference materials like books and notes, find the relevant sections related to a question, and then compose an answer based on that information. This results in far more accurate and reliable answers.

### The Core Working Principle of RAG

RAG operates on a three-step flow: **Retrieval**, **Augmentation**, and **Generation**. This is broadly divided into two main processes: preparing the knowledge and generating the answer.

* **Phase 1: Data Indexing (Offline Preparation)**
    This is the process of creating the "textbook" for the LLM. The system ingests external knowledge sources like PDFs, websites, and internal documents, splits them into meaningful small chunks, and converts the semantic meaning of each chunk into a vector (an array of numbers). These vectors are then stored and indexed in a specialized **vector database**, which acts as a "knowledge warehouse." This entire process is completed before any user asks a question.

* **Phase 2: Retrieve, Augment, Generate (Online Execution)**
    When a user's query comes in, the following three actions occur in real-time:
    1.  **Retrieval**: The system first analyzes the user's query and finds the most relevant chunks of information from the prepared knowledge warehouse. This is like finding the right page to reference in an open-book exam.
    2.  **Augmentation**: This is the core of RAG. The system combines the **retrieved information (context)** with the **user's original query** into a single, new prompt. In other words, it's a process of making the request to the LLM "smarter" by framing it as, "Based on this reference material, please answer the following question." This enhanced prompt maximizes the LLM's performance.
    3.  **Generation**: Finally, the "augmented prompt" is passed to the LLM to create the final answer. Because the LLM bases its response on the provided reference material, it generates an accurate and reliable result without hallucination.

### Detailed RAG System Workflow

The working principle above is implemented through the following specific workflow.

#### 1. Indexing: Preparing the Knowledge Warehouse 📚

1.  **Load and Split Data**: Ingest all the information the system needs to know (PDFs, websites, etc.) and split it into smaller, meaningful chunks.
2.  **Embedding**: Use an **embedding model** to convert each data chunk into a numerical vector that captures its semantic meaning.
3.  **Storing**: Store the resulting vectors and their corresponding original text chunks in a **vector database** so they can be retrieved quickly.

#### 2. Retrieval & Generation: Answering in Real-Time 💬

1.  **Query Embedding**: The user's query is also converted into a vector using the same embedding model used during indexing.
2.  **Information Retrieval**: The system searches the vector database for the data chunks whose vectors are most similar to the query vector.
3.  **Prompt Augmentation**: The retrieved data chunks (the context) are combined with the original user query to create a final, comprehensive prompt for the LLM.
4.  **Answer Generation**: The augmented prompt is sent to the LLM, which then provides a fact-based, accurate answer based on the given context.

### Key Applications of RAG

Because the RAG architecture is highly effective at generating reliable answers based on specific domain data, it's used as a core technology in the following broad application areas.

* **Q&A Chatbots**:
    By integrating RAG into a chatbot, it can provide accurate, real-time answers based on a company's internal documents or the latest knowledge base. This plays a key role in automating customer support and website lead follow-up, allowing the chatbot to understand user intent and resolve issues quickly.

* **Intelligent Search Augmentation**:
    While traditional search engines stop at listing a series of documents, a search engine integrated with RAG can provide a direct, summarized answer to a user's question alongside the results. This dramatically improves the information retrieval experience, as users can get the information they need instantly without having to click through multiple documents.

* **Internal Knowledge Engine**:
    Vast amounts of internal company data, such as HR policies, compliance regulations, and technical specifications, can be used as a reference source for an LLM. Employees can ask questions in natural language, like "What is our vacation policy?" or "What are the key points of the new security protocol?", and receive verified answers based on internal data through the RAG system.

### Design Considerations for Software Engineers

To successfully build a RAG system, engineers must carefully consider the following points:

* **Chunking Strategy**: How should documents be split? Chunks that are too small may lose context, while chunks that are too large can harm retrieval accuracy and increase costs.
* **Embedding Model Selection**: Which embedding model should be used? Performance, speed, cost, and supported languages differ between models.
* **Retrieval Quality**: How accurate is the retrieved information? Quality can be improved by using **hybrid search** (combining keyword and vector search) or by adding a **re-ranking** model to sort the search results.
* **System Scalability and Cost**: How will the system scale as data volumes and user requests increase? The performance of the vector database and LLM API call costs must be considered holistically.
* **Evaluation**: How will the RAG system's performance be measured? It's essential to build an evaluation pipeline that can measure the precision of retrieval, the faithfulness of the generated answer, and the relevance of the final response.

In conclusion, RAG is not just a single technology but a **sophisticated software system** that combines data pipelines, search algorithms, and LLM integration. A successful RAG implementation depends on thoughtful engineering and continuous optimization.

---

## Retrieval-Augmented Generation (RAG)

대규모 언어 모델(LLM)은 놀라운 성능을 보여주며 많은 서비스의 핵심 기술로 자리 잡았습니다. 하지만 LLM에는 근본적인 한계가 존재합니다. 바로 **'지식 단절(Knowledge Cut-off)'** 과 **'환각(Hallucination)'** 현상입니다. 모델이 훈련된 시점 이후의 최신 정보는 알지 못하며, 사실이 아닌 내용을 그럴듯하게 생성하기도 합니다.

소프트웨어 엔지니어의 관점에서 이 문제는 어떻게 해결할 수 있을까요? 오늘 소개할 **RAG(Retrieval-Augmented Generation)** 는 외부의 지식 소스를 동적으로 활용하여 LLM의 한계를 극복하는 강력하고 효율적인 아키텍처입니다.

### RAG란 무엇인가? "오픈북 시험"으로 이해하기 📝

RAG를 가장 쉽게 이해하는 방법은 '오픈북 시험'에 비유하는 것입니다.

* **일반적인 LLM**: 학생이 오직 자신의 머릿속에 있는 지식만으로 시험을 보는 **'클로즈드북 시험'** 과 같습니다. 아는 것은 막힘없이 풀지만, 모르는 내용은 상상해서 답안을 채울 수밖에 없습니다. (환각)
* **RAG 기반 LLM**: 학생이 시험을 볼 때 참고 자료(책, 노트)를 옆에 두고, 질문과 관련된 부분을 빠르게 찾아본 뒤 그 정보를 바탕으로 답안을 작성하는 **'오픈북 시험'** 과 같습니다. 훨씬 더 정확하고 신뢰도 높은 답변이 가능해집니다.

### RAG의 핵심 작동 원리

RAG는 **검색(Retrieval)**, **증강(Augmentation)**, **생성(Generation)** 이라는 3단계의 흐름으로 작동합니다. 이는 크게 '지식 준비'와 '답변 생성'의 두 과정으로 나뉩니다.

* **1단계: 데이터 인덱싱 (Offline Indexing - 준비)**
    이것은 LLM을 위한 '참고서'를 만드는 과정입니다. 시스템은 사전에 PDF, 웹사이트, 내부 문서 등 외부 지식 소스를 가져와 의미 있는 작은 단위로 나눕니다. 그 후, 각 데이터 조각의 의미를 벡터(숫자 배열)로 변환하여 **벡터 데이터베이스**라는 특수한 '지식 창고'에 저장하고 색인(Index)을 달아둡니다. 이 모든 과정은 사용자가 질문하기 전에 미리 이루어집니다.

* **2단계: 검색, 증강, 생성 (Online R-A-G - 실행)**
    사용자의 질문이 들어오면 실시간으로 다음 세 가지 행동이 순차적으로 일어납니다.
    1.  **검색 (Retrieval)**: 시스템은 먼저 사용자의 질문을 분석하여, 준비된 지식 창고에서 가장 관련성 높은 정보 조각들을 찾아냅니다. 이것이 '오픈북 시험'에서 참고할 페이지를 찾는 과정과 같습니다.
    2.  **증강 (Augmentation)**: 이 단계가 RAG의 핵심입니다. 시스템은 앞에서 **검색된 정보(Context)** 와 **사용자의 원본 질문**을 하나의 새로운 프롬프트로 결합합니다. 즉, LLM에게 "이 참고 자료를 바탕으로, 다음 질문에 답해줘"라는 식으로 요청문을 '더 똑똑하게' 만드는 과정입니다. 이렇게 강화된 프롬프트가 LLM의 성능을 극대화합니다.
    3.  **생성 (Generation)**: 마지막으로, '증강된 프롬프트'를 LLM에게 전달하여 최종 답변을 만듭니다. LLM은 주어진 참고 자료에 기반하여 답변하므로, 환각 없이 정확하고 신뢰도 높은 결과를 생성하게 됩니다.

### RAG 시스템의 세부 워크플로우

위의 작동 원리는 구체적으로 다음과 같은 워크플로우를 통해 구현됩니다.

#### 1. 인덱싱 (Indexing): 지식 창고 준비하기 📚

1.  **데이터 로드 및 분할**: 시스템이 알아야 할 모든 정보(PDF, 웹사이트, 내부 문서 등)를 불러와 의미 있는 작은 단위(Chunk)로 나눕니다.
2.  **임베딩 (Embedding)**: 분할된 각 데이터 조각을 **임베딩 모델**을 사용하여 텍스트의 '의미'를 담은 숫자 벡터(Vector)로 변환합니다.
3.  **저장 (Storing)**: 변환된 벡터와 원본 텍스트 조각을 **벡터 데이터베이스(Vector Database)** 에 저장하여 언제든 빠르게 검색할 수 있도록 준비합니다.

#### 2. 검색 및 생성 (Retrieval & Generation): 실시간 답변 생성 💬

1.  **질문 분석 (Query Embedding)**: 사용자의 질문 또한 인덱싱 때와 동일한 임베딩 모델을 사용하여 벡터로 변환합니다.
2.  **정보 검색 (Retrieval)**: 질문 벡터와 가장 유사한 의미를 가진 데이터 조각들을 벡터 데이터베이스에서 검색합니다.
3.  **프롬프트 강화 (Augmentation)**: 검색된 데이터 조각들(문맥 정보)을 사용자의 원본 질문과 결합하여 LLM에게 전달할 최종 프롬프트를 만듭니다.
4.  **답변 생성 (Generation)**: 강화된 프롬프트를 받은 LLM은 주어진 문맥을 바탕으로 질문에 답하여, 환각 없이 사실에 기반한 정확한 답변을 내놓게 됩니다.

### RAG의 주요 활용 분야

RAG 아키텍처는 특정 도메인의 데이터를 기반으로 신뢰도 높은 답변을 생성하는 데 매우 효과적이므로, 다음과 같은 큰 틀의 분야에서 핵심 기술로 사용됩니다.

* **질의응답(Q&A) 챗봇**:
    RAG를 챗봇에 통합하면, 기업의 내부 문서나 최신 지식 베이스를 기반으로 정확한 답변을 실시간으로 제공할 수 있습니다. 이는 고객 지원 자동화, 웹사이트 리드 후속 조치 등에서 사용자가 질문의 의도를 파악하고 신속하게 문제를 해결하는 데 핵심적인 역할을 합니다.

* **지능형 검색 증강 (Intelligent Search Augmentation)**:
    전통적인 검색 엔진이 문서 목록을 나열하는 데 그쳤다면, RAG를 통합한 검색 엔진은 사용자의 질문에 대한 직접적인 요약 답변을 생성하여 함께 제공합니다. 이를 통해 사용자는 여러 문서를 일일이 확인할 필요 없이 필요한 정보를 즉시 얻을 수 있어 정보 검색 경험이 획기적으로 개선됩니다.

* **내부 데이터 기반 지식 엔진 (Internal Knowledge Engine)**:
    기업의 HR 정책, 규정 준수, 기술 사양과 같은 방대한 내부 데이터를 LLM의 참고 자료로 활용할 수 있습니다. 직원들은 "연차 사용 규정이 어떻게 돼?" 또는 "새로운 보안 정책의 주요 내용이 뭐야?"와 같은 질문을 자연어로 하고, RAG 시스템을 통해 사내 데이터에 기반한 검증된 답변을 손쉽게 얻을 수 있습니다.

### 소프트웨어 엔지니어의 RAG 설계 시 고려사항

RAG 시스템을 성공적으로 구축하기 위해 엔지니어는 다음과 같은 점들을 깊이 고민해야 합니다.

* **Chunking 전략**: 문서를 어떻게 나눌 것인가? 너무 작게 나누면 문맥이 손실되고, 너무 크게 나누면 검색 정확도와 비용에 문제가 생깁니다.
* **임베딩 모델 선택**: 어떤 임베딩 모델을 사용할 것인가? 모델의 성능, 속도, 비용, 그리고 지원하는 언어가 모두 다릅니다.
* **검색(Retrieval) 품질**: 검색된 결과가 얼마나 정확한가? 키워드 검색과 벡터 검색을 결합한 **하이브리드 검색**이나 검색 결과를 다시 정렬하는 **재랭킹** 모델을 추가하여 품질을 높일 수 있습니다.
* **시스템 확장성 및 비용**: 데이터가 많아지고 사용자 요청이 늘어날 때 시스템이 어떻게 확장될 수 있는가? 벡터 DB의 성능, LLM API 호출 비용 등을 종합적으로 고려해야 합니다.
* **평가(Evaluation)**: RAG 시스템의 성능을 어떻게 측정할 것인가? 검색의 정확성, 생성된 답변의 충실도, 최종 답변의 관련성 등을 측정할 평가 파이프라인 구축은 필수적입니다.

결론적으로 RAG는 단순히 하나의 기술이 아니라, 데이터 파이프라인, 검색 알고리즘, LLM 연동이 결합된 **정교한 소프트웨어 시스템**입니다. 성공적인 RAG 구현은 이러한 엔지니어링적 고민과 최적화 과정에 달려 있습니다.
---
title: Retrieval-Augmented Generation (RAG)
tags: RAG
key: page-rag
categories: [AI, GenAI]
author: hyoeun
math: true
mathjax_autoNumber: true
---

## Retrieval-Augmented Generation (RAG)

Large Language Models (LLMs) have demonstrated incredible capabilities, becoming a core technology for many services. However, LLMs have fundamental limitations, namely the **"Knowledge Cut-off"** and **"Hallucination"** phenomena. Models are unaware of information beyond their training date and can confidently generate plausible-sounding but false information.

From a software engineer's perspective, how can we solve this problem? The answer lies in **Retrieval-Augmented Generation (RAG)**, a powerful and efficient architecture that overcomes the limitations of LLMs by dynamically leveraging external knowledge sources.

### What is RAG? Understanding it with an "Open-Book Exam" Analogy ğŸ“

The easiest way to understand RAG is to compare it to an open-book exam.

* **A standard LLM** is like a student taking a **"closed-book exam."** The student can answer questions they know flawlessly but has no choice but to guess or imagine answers for things they don't know (leading to hallucination).
* **A RAG-based LLM** is like a student taking an **"open-book exam."** The student can reference materials like books and notes, find the relevant sections related to a question, and then compose an answer based on that information. This results in far more accurate and reliable answers.

### The Core Working Principle of RAG

RAG operates on a three-step flow: **Retrieval**, **Augmentation**, and **Generation**. This is broadly divided into two main processes: preparing the knowledge and generating the answer.

* **Phase 1: Data Indexing (Offline Preparation)**
    This is the process of creating the "textbook" for the LLM. The system ingests external knowledge sources like PDFs, websites, and internal documents, splits them into meaningful small chunks, and converts the semantic meaning of each chunk into a vector (an array of numbers). These vectors are then stored and indexed in a specialized **vector database**, which acts as a "knowledge warehouse." This entire process is completed before any user asks a question.

* **Phase 2: Retrieve, Augment, Generate (Online Execution)**
    When a user's query comes in, the following three actions occur in real-time:
    1.  **Retrieval**: The system first analyzes the user's query and finds the most relevant chunks of information from the prepared knowledge warehouse. This is like finding the right page to reference in an open-book exam.
    2.  **Augmentation**: This is the core of RAG. The system combines the **retrieved information (context)** with the **user's original query** into a single, new prompt. In other words, it's a process of making the request to the LLM "smarter" by framing it as, "Based on this reference material, please answer the following question." This enhanced prompt maximizes the LLM's performance.
    3.  **Generation**: Finally, the "augmented prompt" is passed to the LLM to create the final answer. Because the LLM bases its response on the provided reference material, it generates an accurate and reliable result without hallucination.

### Detailed RAG System Workflow

The working principle above is implemented through the following specific workflow.

#### 1. Indexing: Preparing the Knowledge Warehouse ğŸ“š

1.  **Load and Split Data**: Ingest all the information the system needs to know (PDFs, websites, etc.) and split it into smaller, meaningful chunks.
2.  **Embedding**: Use an **embedding model** to convert each data chunk into a numerical vector that captures its semantic meaning.
3.  **Storing**: Store the resulting vectors and their corresponding original text chunks in a **vector database** so they can be retrieved quickly.

#### 2. Retrieval & Generation: Answering in Real-Time ğŸ’¬

1.  **Query Embedding**: The user's query is also converted into a vector using the same embedding model used during indexing.
2.  **Information Retrieval**: The system searches the vector database for the data chunks whose vectors are most similar to the query vector.
3.  **Prompt Augmentation**: The retrieved data chunks (the context) are combined with the original user query to create a final, comprehensive prompt for the LLM.
4.  **Answer Generation**: The augmented prompt is sent to the LLM, which then provides a fact-based, accurate answer based on the given context.

### Key Applications of RAG

Because the RAG architecture is highly effective at generating reliable answers based on specific domain data, it's used as a core technology in the following broad application areas.

* **Q&A Chatbots**:
    By integrating RAG into a chatbot, it can provide accurate, real-time answers based on a company's internal documents or the latest knowledge base. This plays a key role in automating customer support and website lead follow-up, allowing the chatbot to understand user intent and resolve issues quickly.

* **Intelligent Search Augmentation**:
    While traditional search engines stop at listing a series of documents, a search engine integrated with RAG can provide a direct, summarized answer to a user's question alongside the results. This dramatically improves the information retrieval experience, as users can get the information they need instantly without having to click through multiple documents.

* **Internal Knowledge Engine**:
    Vast amounts of internal company data, such as HR policies, compliance regulations, and technical specifications, can be used as a reference source for an LLM. Employees can ask questions in natural language, like "What is our vacation policy?" or "What are the key points of the new security protocol?", and receive verified answers based on internal data through the RAG system.

### Design Considerations for Software Engineers

To successfully build a RAG system, engineers must carefully consider the following points:

* **Chunking Strategy**: How should documents be split? Chunks that are too small may lose context, while chunks that are too large can harm retrieval accuracy and increase costs.
* **Embedding Model Selection**: Which embedding model should be used? Performance, speed, cost, and supported languages differ between models.
* **Retrieval Quality**: How accurate is the retrieved information? Quality can be improved by using **hybrid search** (combining keyword and vector search) or by adding a **re-ranking** model to sort the search results.
* **System Scalability and Cost**: How will the system scale as data volumes and user requests increase? The performance of the vector database and LLM API call costs must be considered holistically.
* **Evaluation**: How will the RAG system's performance be measured? It's essential to build an evaluation pipeline that can measure the precision of retrieval, the faithfulness of the generated answer, and the relevance of the final response.

In conclusion, RAG is not just a single technology but a **sophisticated software system** that combines data pipelines, search algorithms, and LLM integration. A successful RAG implementation depends on thoughtful engineering and continuous optimization.

---

## Retrieval-Augmented Generation (RAG)

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë†€ë¼ìš´ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©° ë§ì€ ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ê¸°ìˆ ë¡œ ìë¦¬ ì¡ì•˜ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ LLMì—ëŠ” ê·¼ë³¸ì ì¸ í•œê³„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ë°”ë¡œ **'ì§€ì‹ ë‹¨ì ˆ(Knowledge Cut-off)'** ê³¼ **'í™˜ê°(Hallucination)'** í˜„ìƒì…ë‹ˆë‹¤. ëª¨ë¸ì´ í›ˆë ¨ëœ ì‹œì  ì´í›„ì˜ ìµœì‹  ì •ë³´ëŠ” ì•Œì§€ ëª»í•˜ë©°, ì‚¬ì‹¤ì´ ì•„ë‹Œ ë‚´ìš©ì„ ê·¸ëŸ´ë“¯í•˜ê²Œ ìƒì„±í•˜ê¸°ë„ í•©ë‹ˆë‹¤.

ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ì˜ ê´€ì ì—ì„œ ì´ ë¬¸ì œëŠ” ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆì„ê¹Œìš”? ì˜¤ëŠ˜ ì†Œê°œí•  **RAG(Retrieval-Augmented Generation)** ëŠ” ì™¸ë¶€ì˜ ì§€ì‹ ì†ŒìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ LLMì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ëŠ” ê°•ë ¥í•˜ê³  íš¨ìœ¨ì ì¸ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.

### RAGë€ ë¬´ì—‡ì¸ê°€? "ì˜¤í”ˆë¶ ì‹œí—˜"ìœ¼ë¡œ ì´í•´í•˜ê¸° ğŸ“

RAGë¥¼ ê°€ì¥ ì‰½ê²Œ ì´í•´í•˜ëŠ” ë°©ë²•ì€ 'ì˜¤í”ˆë¶ ì‹œí—˜'ì— ë¹„ìœ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

* **ì¼ë°˜ì ì¸ LLM**: í•™ìƒì´ ì˜¤ì§ ìì‹ ì˜ ë¨¸ë¦¿ì†ì— ìˆëŠ” ì§€ì‹ë§Œìœ¼ë¡œ ì‹œí—˜ì„ ë³´ëŠ” **'í´ë¡œì¦ˆë“œë¶ ì‹œí—˜'** ê³¼ ê°™ìŠµë‹ˆë‹¤. ì•„ëŠ” ê²ƒì€ ë§‰í˜ì—†ì´ í’€ì§€ë§Œ, ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ìƒìƒí•´ì„œ ë‹µì•ˆì„ ì±„ìš¸ ìˆ˜ë°–ì— ì—†ìŠµë‹ˆë‹¤. (í™˜ê°)
* **RAG ê¸°ë°˜ LLM**: í•™ìƒì´ ì‹œí—˜ì„ ë³¼ ë•Œ ì°¸ê³  ìë£Œ(ì±…, ë…¸íŠ¸)ë¥¼ ì˜†ì— ë‘ê³ , ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¶€ë¶„ì„ ë¹ ë¥´ê²Œ ì°¾ì•„ë³¸ ë’¤ ê·¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µì•ˆì„ ì‘ì„±í•˜ëŠ” **'ì˜¤í”ˆë¶ ì‹œí—˜'** ê³¼ ê°™ìŠµë‹ˆë‹¤. í›¨ì”¬ ë” ì •í™•í•˜ê³  ì‹ ë¢°ë„ ë†’ì€ ë‹µë³€ì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.

### RAGì˜ í•µì‹¬ ì‘ë™ ì›ë¦¬

RAGëŠ” **ê²€ìƒ‰(Retrieval)**, **ì¦ê°•(Augmentation)**, **ìƒì„±(Generation)** ì´ë¼ëŠ” 3ë‹¨ê³„ì˜ íë¦„ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤. ì´ëŠ” í¬ê²Œ 'ì§€ì‹ ì¤€ë¹„'ì™€ 'ë‹µë³€ ìƒì„±'ì˜ ë‘ ê³¼ì •ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

* **1ë‹¨ê³„: ë°ì´í„° ì¸ë±ì‹± (Offline Indexing - ì¤€ë¹„)**
    ì´ê²ƒì€ LLMì„ ìœ„í•œ 'ì°¸ê³ ì„œ'ë¥¼ ë§Œë“œëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ì‚¬ì „ì— PDF, ì›¹ì‚¬ì´íŠ¸, ë‚´ë¶€ ë¬¸ì„œ ë“± ì™¸ë¶€ ì§€ì‹ ì†ŒìŠ¤ë¥¼ ê°€ì ¸ì™€ ì˜ë¯¸ ìˆëŠ” ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. ê·¸ í›„, ê° ë°ì´í„° ì¡°ê°ì˜ ì˜ë¯¸ë¥¼ ë²¡í„°(ìˆ«ì ë°°ì—´)ë¡œ ë³€í™˜í•˜ì—¬ **ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤**ë¼ëŠ” íŠ¹ìˆ˜í•œ 'ì§€ì‹ ì°½ê³ 'ì— ì €ì¥í•˜ê³  ìƒ‰ì¸(Index)ì„ ë‹¬ì•„ë‘¡ë‹ˆë‹¤. ì´ ëª¨ë“  ê³¼ì •ì€ ì‚¬ìš©ìê°€ ì§ˆë¬¸í•˜ê¸° ì „ì— ë¯¸ë¦¬ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.

* **2ë‹¨ê³„: ê²€ìƒ‰, ì¦ê°•, ìƒì„± (Online R-A-G - ì‹¤í–‰)**
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹¤ìŒ ì„¸ ê°€ì§€ í–‰ë™ì´ ìˆœì°¨ì ìœ¼ë¡œ ì¼ì–´ë‚©ë‹ˆë‹¤.
    1.  **ê²€ìƒ‰ (Retrieval)**: ì‹œìŠ¤í…œì€ ë¨¼ì € ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬, ì¤€ë¹„ëœ ì§€ì‹ ì°½ê³ ì—ì„œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ ì¡°ê°ë“¤ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤. ì´ê²ƒì´ 'ì˜¤í”ˆë¶ ì‹œí—˜'ì—ì„œ ì°¸ê³ í•  í˜ì´ì§€ë¥¼ ì°¾ëŠ” ê³¼ì •ê³¼ ê°™ìŠµë‹ˆë‹¤.
    2.  **ì¦ê°• (Augmentation)**: ì´ ë‹¨ê³„ê°€ RAGì˜ í•µì‹¬ì…ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ì•ì—ì„œ **ê²€ìƒ‰ëœ ì •ë³´(Context)** ì™€ **ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸**ì„ í•˜ë‚˜ì˜ ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ë¡œ ê²°í•©í•©ë‹ˆë‹¤. ì¦‰, LLMì—ê²Œ "ì´ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì¤˜"ë¼ëŠ” ì‹ìœ¼ë¡œ ìš”ì²­ë¬¸ì„ 'ë” ë˜‘ë˜‘í•˜ê²Œ' ë§Œë“œëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ê°€ LLMì˜ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
    3.  **ìƒì„± (Generation)**: ë§ˆì§€ë§‰ìœ¼ë¡œ, 'ì¦ê°•ëœ í”„ë¡¬í”„íŠ¸'ë¥¼ LLMì—ê²Œ ì „ë‹¬í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ë§Œë“­ë‹ˆë‹¤. LLMì€ ì£¼ì–´ì§„ ì°¸ê³  ìë£Œì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ë¯€ë¡œ, í™˜ê° ì—†ì´ ì •í™•í•˜ê³  ì‹ ë¢°ë„ ë†’ì€ ê²°ê³¼ë¥¼ ìƒì„±í•˜ê²Œ ë©ë‹ˆë‹¤.

### RAG ì‹œìŠ¤í…œì˜ ì„¸ë¶€ ì›Œí¬í”Œë¡œìš°

ìœ„ì˜ ì‘ë™ ì›ë¦¬ëŠ” êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì›Œí¬í”Œë¡œìš°ë¥¼ í†µí•´ êµ¬í˜„ë©ë‹ˆë‹¤.

#### 1. ì¸ë±ì‹± (Indexing): ì§€ì‹ ì°½ê³  ì¤€ë¹„í•˜ê¸° ğŸ“š

1.  **ë°ì´í„° ë¡œë“œ ë° ë¶„í• **: ì‹œìŠ¤í…œì´ ì•Œì•„ì•¼ í•  ëª¨ë“  ì •ë³´(PDF, ì›¹ì‚¬ì´íŠ¸, ë‚´ë¶€ ë¬¸ì„œ ë“±)ë¥¼ ë¶ˆëŸ¬ì™€ ì˜ë¯¸ ìˆëŠ” ì‘ì€ ë‹¨ìœ„(Chunk)ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
2.  **ì„ë² ë”© (Embedding)**: ë¶„í• ëœ ê° ë°ì´í„° ì¡°ê°ì„ **ì„ë² ë”© ëª¨ë¸**ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ 'ì˜ë¯¸'ë¥¼ ë‹´ì€ ìˆ«ì ë²¡í„°(Vector)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
3.  **ì €ì¥ (Storing)**: ë³€í™˜ëœ ë²¡í„°ì™€ ì›ë³¸ í…ìŠ¤íŠ¸ ì¡°ê°ì„ **ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤(Vector Database)** ì— ì €ì¥í•˜ì—¬ ì–¸ì œë“  ë¹ ë¥´ê²Œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í•©ë‹ˆë‹¤.

#### 2. ê²€ìƒ‰ ë° ìƒì„± (Retrieval & Generation): ì‹¤ì‹œê°„ ë‹µë³€ ìƒì„± ğŸ’¬

1.  **ì§ˆë¬¸ ë¶„ì„ (Query Embedding)**: ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë˜í•œ ì¸ë±ì‹± ë•Œì™€ ë™ì¼í•œ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
2.  **ì •ë³´ ê²€ìƒ‰ (Retrieval)**: ì§ˆë¬¸ ë²¡í„°ì™€ ê°€ì¥ ìœ ì‚¬í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ ë°ì´í„° ì¡°ê°ë“¤ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
3.  **í”„ë¡¬í”„íŠ¸ ê°•í™” (Augmentation)**: ê²€ìƒ‰ëœ ë°ì´í„° ì¡°ê°ë“¤(ë¬¸ë§¥ ì •ë³´)ì„ ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ê³¼ ê²°í•©í•˜ì—¬ LLMì—ê²Œ ì „ë‹¬í•  ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.
4.  **ë‹µë³€ ìƒì„± (Generation)**: ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì€ LLMì€ ì£¼ì–´ì§„ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì—¬, í™˜ê° ì—†ì´ ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ì •í™•í•œ ë‹µë³€ì„ ë‚´ë†“ê²Œ ë©ë‹ˆë‹¤.

### RAGì˜ ì£¼ìš” í™œìš© ë¶„ì•¼

RAG ì•„í‚¤í…ì²˜ëŠ” íŠ¹ì • ë„ë©”ì¸ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°ë„ ë†’ì€ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ë§¤ìš° íš¨ê³¼ì ì´ë¯€ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ í° í‹€ì˜ ë¶„ì•¼ì—ì„œ í•µì‹¬ ê¸°ìˆ ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

* **ì§ˆì˜ì‘ë‹µ(Q&A) ì±—ë´‡**:
    RAGë¥¼ ì±—ë´‡ì— í†µí•©í•˜ë©´, ê¸°ì—…ì˜ ë‚´ë¶€ ë¬¸ì„œë‚˜ ìµœì‹  ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ê³ ê° ì§€ì› ìë™í™”, ì›¹ì‚¬ì´íŠ¸ ë¦¬ë“œ í›„ì† ì¡°ì¹˜ ë“±ì—ì„œ ì‚¬ìš©ìê°€ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ì‹ ì†í•˜ê²Œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.

* **ì§€ëŠ¥í˜• ê²€ìƒ‰ ì¦ê°• (Intelligent Search Augmentation)**:
    ì „í†µì ì¸ ê²€ìƒ‰ ì—”ì§„ì´ ë¬¸ì„œ ëª©ë¡ì„ ë‚˜ì—´í•˜ëŠ” ë° ê·¸ì³¤ë‹¤ë©´, RAGë¥¼ í†µí•©í•œ ê²€ìƒ‰ ì—”ì§„ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ìš”ì•½ ë‹µë³€ì„ ìƒì„±í•˜ì—¬ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ì¼ì¼ì´ í™•ì¸í•  í•„ìš” ì—†ì´ í•„ìš”í•œ ì •ë³´ë¥¼ ì¦‰ì‹œ ì–»ì„ ìˆ˜ ìˆì–´ ì •ë³´ ê²€ìƒ‰ ê²½í—˜ì´ íšê¸°ì ìœ¼ë¡œ ê°œì„ ë©ë‹ˆë‹¤.

* **ë‚´ë¶€ ë°ì´í„° ê¸°ë°˜ ì§€ì‹ ì—”ì§„ (Internal Knowledge Engine)**:
    ê¸°ì—…ì˜ HR ì •ì±…, ê·œì • ì¤€ìˆ˜, ê¸°ìˆ  ì‚¬ì–‘ê³¼ ê°™ì€ ë°©ëŒ€í•œ ë‚´ë¶€ ë°ì´í„°ë¥¼ LLMì˜ ì°¸ê³  ìë£Œë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§ì›ë“¤ì€ "ì—°ì°¨ ì‚¬ìš© ê·œì •ì´ ì–´ë–»ê²Œ ë¼?" ë˜ëŠ” "ìƒˆë¡œìš´ ë³´ì•ˆ ì •ì±…ì˜ ì£¼ìš” ë‚´ìš©ì´ ë­ì•¼?"ì™€ ê°™ì€ ì§ˆë¬¸ì„ ìì—°ì–´ë¡œ í•˜ê³ , RAG ì‹œìŠ¤í…œì„ í†µí•´ ì‚¬ë‚´ ë°ì´í„°ì— ê¸°ë°˜í•œ ê²€ì¦ëœ ë‹µë³€ì„ ì†ì‰½ê²Œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ì˜ RAG ì„¤ê³„ ì‹œ ê³ ë ¤ì‚¬í•­

RAG ì‹œìŠ¤í…œì„ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•í•˜ê¸° ìœ„í•´ ì—”ì§€ë‹ˆì–´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì ë“¤ì„ ê¹Šì´ ê³ ë¯¼í•´ì•¼ í•©ë‹ˆë‹¤.

* **Chunking ì „ëµ**: ë¬¸ì„œë¥¼ ì–´ë–»ê²Œ ë‚˜ëˆŒ ê²ƒì¸ê°€? ë„ˆë¬´ ì‘ê²Œ ë‚˜ëˆ„ë©´ ë¬¸ë§¥ì´ ì†ì‹¤ë˜ê³ , ë„ˆë¬´ í¬ê²Œ ë‚˜ëˆ„ë©´ ê²€ìƒ‰ ì •í™•ë„ì™€ ë¹„ìš©ì— ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤.
* **ì„ë² ë”© ëª¨ë¸ ì„ íƒ**: ì–´ë–¤ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•  ê²ƒì¸ê°€? ëª¨ë¸ì˜ ì„±ëŠ¥, ì†ë„, ë¹„ìš©, ê·¸ë¦¬ê³  ì§€ì›í•˜ëŠ” ì–¸ì–´ê°€ ëª¨ë‘ ë‹¤ë¦…ë‹ˆë‹¤.
* **ê²€ìƒ‰(Retrieval) í’ˆì§ˆ**: ê²€ìƒ‰ëœ ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ ì •í™•í•œê°€? í‚¤ì›Œë“œ ê²€ìƒ‰ê³¼ ë²¡í„° ê²€ìƒ‰ì„ ê²°í•©í•œ **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**ì´ë‚˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì •ë ¬í•˜ëŠ” **ì¬ë­í‚¹** ëª¨ë¸ì„ ì¶”ê°€í•˜ì—¬ í’ˆì§ˆì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* **ì‹œìŠ¤í…œ í™•ì¥ì„± ë° ë¹„ìš©**: ë°ì´í„°ê°€ ë§ì•„ì§€ê³  ì‚¬ìš©ì ìš”ì²­ì´ ëŠ˜ì–´ë‚  ë•Œ ì‹œìŠ¤í…œì´ ì–´ë–»ê²Œ í™•ì¥ë  ìˆ˜ ìˆëŠ”ê°€? ë²¡í„° DBì˜ ì„±ëŠ¥, LLM API í˜¸ì¶œ ë¹„ìš© ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
* **í‰ê°€(Evaluation)**: RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì–´ë–»ê²Œ ì¸¡ì •í•  ê²ƒì¸ê°€? ê²€ìƒ‰ì˜ ì •í™•ì„±, ìƒì„±ëœ ë‹µë³€ì˜ ì¶©ì‹¤ë„, ìµœì¢… ë‹µë³€ì˜ ê´€ë ¨ì„± ë“±ì„ ì¸¡ì •í•  í‰ê°€ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•ì€ í•„ìˆ˜ì ì…ë‹ˆë‹¤.

ê²°ë¡ ì ìœ¼ë¡œ RAGëŠ” ë‹¨ìˆœíˆ í•˜ë‚˜ì˜ ê¸°ìˆ ì´ ì•„ë‹ˆë¼, ë°ì´í„° íŒŒì´í”„ë¼ì¸, ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜, LLM ì—°ë™ì´ ê²°í•©ëœ **ì •êµí•œ ì†Œí”„íŠ¸ì›¨ì–´ ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤. ì„±ê³µì ì¸ RAG êµ¬í˜„ì€ ì´ëŸ¬í•œ ì—”ì§€ë‹ˆì–´ë§ì  ê³ ë¯¼ê³¼ ìµœì í™” ê³¼ì •ì— ë‹¬ë ¤ ìˆìŠµë‹ˆë‹¤.